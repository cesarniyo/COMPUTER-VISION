{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FellowAI Competition",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cesarniyo/Competition/blob/master/FellowAI_Competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "# FellowAI Competition \n",
        "\n",
        "*by Cesar Niyomugabo:Fellow Team Austin*\n",
        "     \n",
        "     \n",
        " ## Property  analysis approach     \n",
        " \n",
        " \n",
        " In this project we combined our trained model on flooring damage/ decay detection with other team's models to detect any damaged floors, overgrown plants, exposed wires and electrical unsafely handled. Our detailed approach is explained in following steps: \n",
        " \n",
        "\n",
        "\n",
        "**Step 1: Download  the videon from youtube**\n",
        "\n",
        "On this step we do access the video from the  Youtube link provided\n",
        "\n",
        "\n",
        "**Step 2: Reading in the video and feed frames one by one to the models for predictions.**\n",
        "\n",
        "- **Flooring model**\n",
        "\n",
        " >The flooring model predicts two classes (Damaged and Normal). If Damage detected, result are displayed on the frame, otherwise nothing displayed.\n",
        "\n",
        "\n",
        "-  **Landscape model**:\n",
        "\n",
        " >The Landscape model predicts two categories (clean, overgrown).If overgrown landscape detected, we display the result on the frame, if  'clean' predicted nothing displayed\n",
        "\n",
        "\n",
        "- **childproofed-electric model**:\n",
        "\n",
        " >The childproofed-electric model predicts 3 categories (childproofed, open plugs, open wires).If open wires predicted with model confidence over 98% we display the result on the frame, otherwise nothing detected.\n",
        "\n",
        "- **electrical safety model**:\n",
        "\n",
        " >The electrical safely model predicts 2 categories (unsafe, safe).If unsafe predicted with model confidence over 95%,  we display the result on the frame, otherwise nothing detected.\n",
        " \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "**Step 3: Prediction result analysis**\n",
        "\n",
        "On this step we compare the models prediction results to real content of the frame to get the strength of each model(Detailed in the conclusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_ebWWY-_7pT",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the video from youtube"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJSQtDMKRUcA",
        "colab_type": "code",
        "outputId": "31a3882e-1f07-4f45-8725-45321f9efc3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "!pip install pytube\n",
        "import pytube\n",
        "video_url='https://www.youtube.com/watch?v=mdHm8hAwVUA&feature=youtu.be'\n",
        "youtube=pytube.YouTube(video_url)#Pass the url here\n",
        "video=youtube.streams.first()\n",
        "!mkdir '/content/video'          # create a folder to hold the video on google colab\n",
        "video.download('/content/video') # Path, where to video download\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytube in /usr/local/lib/python3.6/dist-packages (9.5.2)\n",
            "mkdir: cannot create directory ‘/content/video’: File exists\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/video/Arshak’s house of defects.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar99EBSvAYVY",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HZmunxnA8UA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from fastai.vision import *\n",
        "#from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5h8wwsSBXpu",
        "colab_type": "text"
      },
      "source": [
        "## Unzipping the folder containing the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUDKH9-wBnhK",
        "colab_type": "code",
        "outputId": "af6ffed6-2b35-4a7b-98ec-d8ba179a856b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "from zipfile import ZipFile\n",
        "file_name= \"models.zip\"\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM7olBV1B4X3",
        "colab_type": "text"
      },
      "source": [
        "##  Video Processing (Prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIQ-XkE6ykKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def prediction(frame,path):  \n",
        "       learn = load_learner(path)\n",
        "       learn = learn.to_fp32()\n",
        "       frame_copy =  frame.copy()\n",
        "       frame_resized = cv2.resize(frame_copy,(224,224))        \n",
        "       t = torch.tensor(np.ascontiguousarray(np.flip( frame_resized, 2)).transpose(2,0,1)).float()/255\n",
        "       img = Image(t) # fastai.vision.Image, not PIL.Image       \n",
        "       p = learn.predict(img) #pred_class,pred_idx,outputs = learn.predict(img)\n",
        "       #print(p[0])\n",
        "       return p  \n",
        "  \n",
        " \n",
        "  \n",
        "  \n",
        "def VideoProcessing(video,path1,path2,path3,path4): \n",
        "  \n",
        "      # Create a VideoCapture object and read from input file\n",
        "      # If the input is the camera, pass 0 instead of the video file name\n",
        "      cap = cv2.VideoCapture(video)\n",
        "\n",
        "      # Check if camera opened successfully\n",
        "      if (cap.isOpened()== False): \n",
        "          print(\"Error opening video stream or file\")\n",
        "\n",
        "      # Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
        "      # We convert the resolutions from float to integer.\n",
        "      frame_width = int(cap.get(3))\n",
        "      frame_height = int(cap.get(4))\n",
        "      print('...video Processing in Progress')\n",
        "      \n",
        "      x,y=0,620\n",
        "      w=300\n",
        "      h=50\n",
        "\n",
        "      # Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
        "      out = cv2.VideoWriter('AustinTeam.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
        "\n",
        "      \n",
        "      #Read until video is completed\n",
        "      while(cap.isOpened()):\n",
        "          # Capture frame-by-frame\n",
        "          ret, frame = cap.read()\n",
        "          if ret == True:\n",
        "            \n",
        "              p1=prediction(frame,path1)#flooring\n",
        "              p2=prediction(frame,path2)#overgrownplants       \n",
        "              p3=prediction(frame,path3)#childproofelectrical  \n",
        "              p4=prediction(frame,path4)#electrical\n",
        "              #print(p1)\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "              # Write the frame into the file 'output.avi'\n",
        "              if str(p1[0])=='Damaged':\n",
        "                 cv2.rectangle(frame,(0, 620),(300,590),(0,255,0),4)#cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),4)\n",
        "                 #cv2.putText(image, 'name', starting point, font, fontScale, color, thickness, cv2.LINE_AA)\n",
        "                 cv2.putText(frame, 'Damaged-Floor', (0, 620), cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 0, 255), 2, cv2.LINE_AA, False)\n",
        "                 out.write(frame)\n",
        "\n",
        "              if str(p2[0])=='overgrown': \n",
        "                 cv2.rectangle(frame,(0,520),(300,490),(0,255,0),4)\n",
        "                 cv2.putText(frame, 'overgrown-plants', (0,520), cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 0, 255), 2, cv2.LINE_AA, False)\n",
        "                 out.write(frame)\n",
        "\n",
        "\n",
        "              if str(p3[0])=='open wires' and p3[2][2]>=0.98 : #childproofed #open plugs #open wires\n",
        "                text = f'{p3[0]}'\n",
        "                cv2.rectangle(frame,(0,420),(300,390),(0,255,0),4)#cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),4)\n",
        "                cv2.putText(frame, text, (0,420), cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 0, 255), 2, cv2.LINE_AA, False)\n",
        "                out.write(frame)\n",
        "\n",
        "              if str(p4[0])=='unsafe'and p4[2][1]>=0.95 : \n",
        "                cv2.rectangle(frame,(0,320),(300,290),(0,255,0),4)\n",
        "                cv2.putText(frame, 'Exposed-electric', (0,320), cv2.FONT_HERSHEY_SIMPLEX, 1,(0, 0, 255), 2, cv2.LINE_AA, False)\n",
        "                out.write(frame)\n",
        "\n",
        "\n",
        "              else:\n",
        "                 out.write(frame)\n",
        "                 #pass \n",
        "\n",
        "              # Write the frame into the file 'austin.avi' \n",
        "              #out.write(frame)\n",
        "\n",
        "              # Display the resulting frame\n",
        "              #cv2.imshow('frame',gray)\n",
        "\n",
        "              # Press Q on keyboard to  exit\n",
        "              if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "                 break\n",
        "\n",
        "          # Break the loop\n",
        "          else: \n",
        "            break\n",
        "\n",
        "      # When everything done, release the video capture object\n",
        "      cap.release()\n",
        "      out.release() \n",
        "\n",
        "      # Closes all the frames\n",
        "      cv2.destroyAllWindows()\n",
        "      return ('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqbHw2egytDj",
        "colab_type": "code",
        "outputId": "362a9618-b782-47ee-b12c-d243400b2c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "path1= '/content/models/flooring'\n",
        "path2= '/content/models/overgrownplants'\n",
        "path3= '/content/models/childproofelectrical'\n",
        "path4= '/content/models/electrical'\n",
        "\n",
        "\n",
        "video='/content/video/Arshak’s house of defects.mp4'\n",
        "VideoProcessing(video,path1,path2,path3,path4)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...video Processing in Progress\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Done'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt-CN_Ly10e3",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "\n",
        "\n",
        " We have combined our model with models from different teams to detect any damage/risk in and around a house. The models we experimented with were able to yeild some good results however we also noticed some short comings in classifying the image properly, bellow are some  of the observation details:\n",
        "\n",
        "**Flooring model**\n",
        "\n",
        "*   True Positives observed :Yes\n",
        "*   False Positives observed :Yes, floors with shadow or differenty lighting are classified as damaged, also floors with stairs at edges are classified as damage as well.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Landscape model**:\n",
        "\n",
        "\n",
        "*   True Positives observed:Yes\n",
        "*   False Positives observed:Yes, this model parfomed well except in some cases were it shows true positives inside the house\n",
        "\n",
        "\n",
        "\n",
        "**childproofed-electric model**:\n",
        "\n",
        "*   True Positives observed:Yes\n",
        "*   False Positives observed:Yes, small tree truncs are classified as open wires\n",
        "\n",
        "\n",
        "\n",
        "**electrical safety model**:\n",
        "\n",
        "*   True Positives observed:Yes\n",
        "*   False Positives observed:Yes, classifying trees as electric hazard.\n",
        "\n",
        "\n",
        "\n",
        "From the above observations we come to the conclusion that the main problem lies on how the models were trained individually earlier. For example, our flooring  damage model, - although we had several damage categories and their respective 'Normal ' categories, most of them did note have the virition of the light and shadows like seen in the video. So, the model sometime fails to classify Normal floor collectly if any shodows or different light intensity on it.  We need to have larger dataset also consider some of the complex cases to train a better model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Notice:some of other team's models we tried to use were note able to be loaded as pickles,so note able to experiment with those.\n"
      ]
    }
  ]
}